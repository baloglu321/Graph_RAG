from llama_index.core import Document, PropertyGraphIndex, Settings
from llama_index.graph_stores.neo4j import Neo4jPropertyGraphStore
from llama_index.llms.ollama import Ollama
from llama_index.embeddings.huggingface import HuggingFaceEmbedding
from llama_index.core.node_parser import JSONNodeParser
from llama_index.core.node_parser import SentenceSplitter
from tqdm import tqdm
import os
import hashlib
import nest_asyncio
import json

nest_asyncio.apply()

CLOUDFLARE_TUNNEL_URL = ".../"
OLLAMA_MODEL_ID = "gemma3:27b"
# ---------------------------------------------------------
# 1. AYARLAR
# ---------------------------------------------------------
JSON_DIR = "./database"  # JSON dosyalarÄ±nÄ±n olduÄŸu klasÃ¶r
STATE_FILE = "file_state.json"  # Hangi dosyanÄ±n iÅŸlendiÄŸini tutan hafÄ±za dosyasÄ±

Settings.llm = Ollama(
    model=OLLAMA_MODEL_ID,
    base_url=CLOUDFLARE_TUNNEL_URL,
    context_window=8192,
    # verbose=True ekleyerek LiteLLM'in HTTP isteklerini gorebilirsiniz.
    request_timeout="3000",
)

Settings.embed_model = HuggingFaceEmbedding(
    model_name="paraphrase-multilingual-mpnet-base-v2"
)

# 2. Neo4j BaÄŸlantÄ±sÄ±
# Åifreni az Ã¶nce belirlediÄŸin ÅŸifreyle deÄŸiÅŸtir
graph_store = Neo4jPropertyGraphStore(
    username="neo4j",
    password="neo4j/your_password",
    url="bolt://localhost:7687",
)


# Mevcut Graph Ä°ndeksini YÃ¼kle (SÄ±fÄ±rdan oluÅŸturmak yerine var olana baÄŸlanÄ±r)
index = PropertyGraphIndex.from_existing(
    property_graph_store=graph_store, embed_model=Settings.embed_model, llm=Settings.llm
)

# ---------------------------------------------------------
# 2. YARDIMCI FONKSÄ°YONLAR (HASH & STATE)
# ---------------------------------------------------------


def calculate_file_hash(filepath):
    """Dosya iÃ§eriÄŸinin MD5 Ã¶zetini Ã§Ä±karÄ±r. Ä°Ã§erik deÄŸiÅŸirse bu Ã¶zet deÄŸiÅŸir."""
    hasher = hashlib.md5()
    with open(filepath, "rb") as f:
        buf = f.read()
        hasher.update(buf)
    return hasher.hexdigest()


def load_state():
    """Ã–nceki Ã§alÄ±ÅŸtÄ±rmadan kalan dosya durumlarÄ±nÄ± yÃ¼kler."""
    if os.path.exists(STATE_FILE):
        with open(STATE_FILE, "r") as f:
            return json.load(f)
    return {}


def save_state(state):
    """GÃ¼ncel dosya durumlarÄ±nÄ± kaydeder."""
    with open(STATE_FILE, "w") as f:
        json.dump(state, f, indent=4)


def clean_old_data_from_graph(filename):
    """
    EÄŸer dosya gÃ¼ncellendiyse, o dosyaya ait ESKÄ° node'larÄ± graph'tan siler.
    Bunu yapmazsak graph'ta duplicate (kopya) veriler oluÅŸur.
    """
    driver = graph_store._driver
    # Cypher sorgusu: KaynaÄŸÄ± (source) bu dosya olan tÃ¼m dÃ¼ÄŸÃ¼mleri sil
    query = """
    MATCH (n) 
    WHERE n.source_file = $filename 
    DETACH DELETE n
    """
    with driver.session() as session:
        session.run(query, filename=filename)
    print(f"ğŸ—‘ï¸  Eski veriler silindi: {filename}")


# ---------------------------------------------------------
# 3. ANA MANTIK (GÃœNCELLEME KONTROLÃœ)
# ---------------------------------------------------------


def process_documents():
    current_state = load_state()
    new_state = current_state.copy()

    # Database klasÃ¶rÃ¼ndeki tÃ¼m .json dosyalarÄ±nÄ± bul
    files = [f for f in os.listdir(JSON_DIR) if f.endswith(".json")]

    files_processed_count = 0

    for filename in files:
        filepath = os.path.join(JSON_DIR, filename)
        current_hash = calculate_file_hash(filepath)

        # KONTROL: Dosya daha Ã¶nce iÅŸlendi mi ve iÃ§eriÄŸi aynÄ± mÄ±?
        if filename in current_state and current_state[filename] == current_hash:
            print(f"â© DeÄŸiÅŸiklik yok, atlanÄ±yor: {filename}")
            continue

        print(f"ğŸ”„ Ä°ÅŸleniyor (Yeni veya DeÄŸiÅŸmiÅŸ): {filename}")

        # 1. AdÄ±m: EÄŸer dosya gÃ¼ncelleniyorsa, Graph'tan eskisini temizle
        if filename in current_state:
            clean_old_data_from_graph(filename)

        # 2. AdÄ±m: DosyayÄ± Oku
        with open(filepath, "r", encoding="utf-8") as f:
            content = f.read()  # JSON'u string olarak okuyoruz

        # 3. AdÄ±m: Document Objesi OluÅŸtur
        splitter = SentenceSplitter(chunk_size=1024, chunk_overlap=50)
        text_chunks = splitter.split_text(content)

        print(
            f"ğŸ“„ {filename} dosyasÄ± {len(text_chunks)} parÃ§aya bÃ¶lÃ¼ndÃ¼. Graph'a iÅŸleniyor..."
        )

        # Her parÃ§ayÄ± Document objesine Ã§evir
        documents = [
            Document(text=chunk, metadata={"source_file": filename})
            for chunk in text_chunks
        ]

        # 4. AdÄ±m: Graph'a Ekle (Insert) - DÃ–NGÃœ Ä°LE
        # index.insert() tek seferde tek belge alÄ±r. Listeyi dÃ¶ngÃ¼ye sokuyoruz.
        # tqdm sayesinde ekranda [=====>     ] ÅŸeklinde ilerleme Ã§ubuÄŸu Ã§Ä±kacak.

        for doc in tqdm(documents, desc=f"ğŸš€ {filename} iÅŸleniyor", unit="chunk"):
            try:
                index.insert(doc)
            except Exception as e:
                print(f"\nâš ï¸ Hata (Bu parÃ§a atlandÄ±): {e}")
                # Hata olsa bile dÃ¶ngÃ¼ devam etsin, tÃ¼m iÅŸlem durmasÄ±n.
                continue

        # 5. AdÄ±m: State'i gÃ¼ncelle
        new_state[filename] = current_hash
        save_state(new_state)
        files_processed_count += 1

    print(f"\nâœ… Ä°ÅŸlem tamamlandÄ±. Toplam gÃ¼ncellenen dosya: {files_processed_count}")


# ---------------------------------------------------------
# 4. Ã‡ALIÅTIR VE TEST ET
# ---------------------------------------------------------
if __name__ == "__main__":
    # KlasÃ¶r yoksa uyar
    if not os.path.exists(JSON_DIR):
        os.makedirs(JSON_DIR)
        print(
            f"ğŸ“ '{JSON_DIR}' klasÃ¶rÃ¼ oluÅŸturuldu. LÃ¼tfen iÃ§ine JSON dosyalarÄ±nÄ± koy."
        )
    else:
        process_documents()

        # Test sorusu (Ä°steÄŸe baÄŸlÄ±)
        # query_engine = index.as_query_engine(include_text=True)
        # print(query_engine.query("VeritabanÄ±ndaki son bilgiler Ä±ÅŸÄ±ÄŸÄ±nda Ã¶zet geÃ§."))
